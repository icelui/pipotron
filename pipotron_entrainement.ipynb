{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"pipotron_entrainement.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rMzMUl-nGL_t","executionInfo":{"status":"ok","timestamp":1615892470858,"user_tz":-60,"elapsed":1118,"user":{"displayName":"François Guérillon","photoUrl":"","userId":"03664453875370395754"}},"outputId":"85745ae0-4567-4875-d12b-464c191ddde3"},"source":["import pandas as pd\n","from google.colab import drive\n","drive.mount('/drive')\n","%cd /drive/MyDrive/data/pipotron/\n","%pwd\n","%ls -al"],"execution_count":78,"outputs":[{"output_type":"stream","text":["Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n","/drive/MyDrive/data/pipotron\n","total 52\n","drwx------ 2 root root  4096 Mar 16 10:48 \u001b[0m\u001b[01;34mdonnees\u001b[0m/\n","drwx------ 8 root root  4096 Mar 16 10:40 \u001b[01;34m.git\u001b[0m/\n","-rw------- 1 root root  1799 Mar 16 10:40 .gitignore\n","-rw------- 1 root root  3522 Mar 16 10:41 git.ipynb\n","drwx------ 3 root root  4096 Mar 16 10:46 \u001b[01;34mmodels\u001b[0m/\n","-rw------- 1 root root 33797 Mar 16 11:00 pipotron_entrainement.ipynb\n","-rw------- 1 root root    79 Mar 16 10:40 README.md\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":108},"id":"zmI30JueGL_4","executionInfo":{"status":"ok","timestamp":1615893060075,"user_tz":-60,"elapsed":494,"user":{"displayName":"François Guérillon","photoUrl":"","userId":"03664453875370395754"}},"outputId":"8d87e59e-cc9a-4dc9-c3b8-ce5fabce3270"},"source":["# Chargement des données :\n","\n","from_full_table = False\n","reviews_filename = \"donnees/review_only.txt\"\n","\n","if from_full_table:\n","  # Chargement du fichier complet, issu du scraping du site larvf.com (données accessibles par abonnement, non publiques):\n","  DF_rvf = pd.read_csv(\"/drive/MyDrive/donnees/larvf_2020-11-24.csv\")\n","  print(\"Nombre de revues textuelles disponibles :\", len(DF_rvf)-DF_rvf['review_text'].isna().sum())\n","\n","  # On sélectionne la colonne qui nous intéresse en retirant les valeurs manquantes, et on fait un mélange aléatoire: \n","  S_reviews = DF_rvf['review_text'][~DF_rvf['review_text'].isna()].sample(frac=1, random_state=5406).reset_index(drop=True)\n","  display(S_reviews.head(4))\n","  \n","  # On enregistre ces données pour ce projet:\n","  S_reviews.to_csv(reviews_filename, header=False, index=False)\n","\n","else:\n","  S_reviews = pd.read_csv(reviews_filename, header=None).iloc[:, 0]\n","  display(S_reviews.head(4))\n"],"execution_count":88,"outputs":[{"output_type":"display_data","data":{"text/plain":["0    Cette cuvée a gagné en élégance et en délicate...\n","1    Bonne trame acide. Légères notes animales à l’...\n","2    Réservé, à la fois pur et nerveux, il s'appuie...\n","3    Un assemblage de meunier et chardonnay, pas d’...\n","Name: 0, dtype: object"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"wRHhbr2tGMAA","executionInfo":{"status":"ok","timestamp":1615893070577,"user_tz":-60,"elapsed":600,"user":{"displayName":"François Guérillon","photoUrl":"","userId":"03664453875370395754"}},"outputId":"7f18a39d-56ec-4171-e1b1-bba4bf10f0ed"},"source":["# Préparation des données:\n","import re\n","\n","# On ajoute un nouveau mot (token) qui a vocation à servir de déclencheur pour générer un nouveau commentaire de dégustation.\n","# On supprime également les informations de date du commentaire, parfois présentes.\n","# Enfin on ajoute un token de fin de séquence pour inciter le modèle à produire des commentaires de taille raisonnable.\n","L_first_tokens = [\"<|review|>\"]\n","L_reviews = [L_first_tokens[0] + \" \" + re.sub(r\"\\([^\\(\\)]*\\.[0-9][0-9][0-9][0-9]\\)\", \"\", review.strip()) for review in list(S_reviews)]\n","L_reviews = [review + \" <|end|>\" for review in L_reviews]\n","\n","# Affichons le résultat de cette préparation sur quelques lignes:\n","for i in range(27,32):\n","  print(L_reviews[i])"],"execution_count":89,"outputs":[{"output_type":"stream","text":["<|review|> On sent des vignes qui ont du fond dans un vin mûr et structuré, qui reste simple en finale. <|end|>\n","<|review|> Un véritable jus de caillou croquant et tonique, un vin plein de répartie, singulier dans l'éclat très ferme de son fruit tendu, d'une rare intensité désaltérante et juteuse. <|end|>\n","<|review|> Il s’exprime sur le fruit noir. Il mêle virilité et élégance, avec une forte empreinte du terroir. <|end|>\n","<|review|> Derrière une fine réduction, il livre une note d’épices et de garrigue. Il lui faut un peu d’air pour libérer son fruit. Il offre une très belle qualité du fruit, du relief avec une jolie assise tannique mais sans dureté. Long en bouche, il possède un beau potentiel de garde et d’évolution. <|end|>\n","<|review|> Les vendanges ont été plus tardives qu’à Haut-Brion, mais le vin conserve une réelle fraîcheur et un fruit sapide. Beaucoup de crémeux avec une fine sucrosité et un boisé doux (60 % de bois neuf). Bouche de grand équilibre avec une saveur saline en finale dans un assemblage à quasi parts égales de merlot et de cabernet-sauvignon. <|end|>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3AoFhEEkGMAB","executionInfo":{"status":"ok","timestamp":1615893089008,"user_tz":-60,"elapsed":498,"user":{"displayName":"François Guérillon","photoUrl":"","userId":"03664453875370395754"}},"outputId":"d6846510-1cad-4e5b-9951-c5b7499ef846"},"source":["# On vérifie le nombre de lignes:\r\n","print(len(L_reviews))\r\n","assert len(L_reviews)==len(DF_rvf)-DF_rvf['review_text'].isna().sum()"],"execution_count":90,"outputs":[{"output_type":"stream","text":["30667\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UK8BQJHyGMAD","executionInfo":{"status":"ok","timestamp":1615893278542,"user_tz":-60,"elapsed":4091,"user":{"displayName":"François Guérillon","photoUrl":"","userId":"03664453875370395754"}},"outputId":"6b3c0ea6-52dc-4c09-f64d-c3c7f5c061a2"},"source":["# On va s'appuyer sur un modèle de générateur GPT-2 (merci OpenAI) pré-entraîné sur la langue française (merci Antoine Louis),\n","# et on utilise la bibliothèque transformers (merci à HuggingFace): \n","!pip install transformers\n","from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n","import tensorflow as tf\n","\n","# On charge le modèle pré-entraîné et son tokenizer associé:\n","model_name = \"antoiloui/belgpt2\"\n","model = TFGPT2LMHeadModel.from_pretrained(model_name)\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","nb_added_tokens = 0"],"execution_count":96,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.3.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"],"name":"stdout"},{"output_type":"stream","text":["All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n","\n","All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at antoiloui/belgpt2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CB98yCiFGMAD","executionInfo":{"status":"ok","timestamp":1615893278544,"user_tz":-60,"elapsed":884,"user":{"displayName":"François Guérillon","photoUrl":"","userId":"03664453875370395754"}},"outputId":"58b5f517-18d7-4f73-853a-4dd71bbbef3a"},"source":["# On ajoute des tokens de déclenchement et de padding au tokenizer, et on prépare le modèle à recevoir ces nouveaux tokens:\n","nb_added_tokens += tokenizer.add_special_tokens({'pad_token': \"<|pad|>\", 'eos_token':\"<|end|>\"})\n","if len(L_first_tokens)>0:\n","    nb_added_tokens += tokenizer.add_special_tokens({'additional_special_tokens': L_first_tokens})\n","print(nb_added_tokens, \"token(s) ajoutés\")\n","\n","_ = model.resize_token_embeddings(tokenizer.vocab_size + nb_added_tokens)"],"execution_count":97,"outputs":[{"output_type":"stream","text":["3 token(s) ajoutés\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ru_n6lEGMAD","executionInfo":{"status":"ok","timestamp":1615893281337,"user_tz":-60,"elapsed":871,"user":{"displayName":"François Guérillon","photoUrl":"","userId":"03664453875370395754"}},"outputId":"5a4c2d81-f6c1-44de-f609-427c051885ed"},"source":["# On vérifie la bonne définition des tokens de padding et de fin de séquence :\n","print(tokenizer.pad_token, \":\", tokenizer.pad_token_id)\n","assert tokenizer.pad_token_id!=None\n","print(tokenizer.eos_token, \":\", tokenizer.eos_token_id)\n","assert tokenizer.eos_token_id!=None"],"execution_count":98,"outputs":[{"output_type":"stream","text":["<|pad|> : 50257\n","<|end|> : 50258\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vTGjSK6Ea46x","executionInfo":{"status":"ok","timestamp":1615893282854,"user_tz":-60,"elapsed":492,"user":{"displayName":"François Guérillon","photoUrl":"","userId":"03664453875370395754"}},"outputId":"f6c24249-0968-4be4-af4f-1dddf023e642"},"source":["# On vérifie qu'un commentaire de vin est inchangé après tokenisation puis détokenisation:\r\n","i = 4449\r\n","print(L_reviews[i])\r\n","print(tokenizer.decode(tokenizer.encode(L_reviews[i])))\r\n","assert L_reviews[i] == tokenizer.decode(tokenizer.encode(L_reviews[i]))"],"execution_count":99,"outputs":[{"output_type":"stream","text":["<|review|> La précision de la matière entretient la force qui se dégage de ce vin. Amples, les tanins encore denses restent intégrés à la matière et soutiennent un fruité généreux. Les notes de cacao portent l'allonge avec puissance. <|end|>\n","<|review|> La précision de la matière entretient la force qui se dégage de ce vin. Amples, les tanins encore denses restent intégrés à la matière et soutiennent un fruité généreux. Les notes de cacao portent l'allonge avec puissance. <|end|>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"grHVIlkJGMAF","executionInfo":{"status":"ok","timestamp":1615893304755,"user_tz":-60,"elapsed":17849,"user":{"displayName":"François Guérillon","photoUrl":"","userId":"03664453875370395754"}},"outputId":"8abb90fb-cd01-44b5-accf-727d79c1770d"},"source":["# On génère un commentaire aléatoire, avant entraînement spécifique (fine tuning) donc non exploitable à ce stade:\n","\n","# TO DO: optimiser si besoin (?) les paramètres de génération aléatoire \n","# cf. https://blog.fastforwardlabs.com/2019/05/29/open-ended-text-generation.html\n","\n","def pipote(max_length=200, skip_special_tokens=True):\n","  input = tokenizer.encode(L_first_tokens[0], return_tensors='tf')\n","  output = model.generate(\n","      input_ids=input,\n","      max_length=max_length,\n","      do_sample=True,\n","      pad_token_id=tokenizer.pad_token_id \n","  )\n","  return tokenizer.decode(output[0], skip_special_tokens=skip_special_tokens)\n","\n","print(pipote())"],"execution_count":100,"outputs":[{"output_type":"stream","text":["ages, les murs ont été arrachés, des vitres ont été brisées... \", se plaint le propriétaire. Depuis l' année 2010, elles ont en effet été multipliées par environ 10 et elles sont prêtes à atteindre 5,3 milliards d' euros d' ici la fin du premier semestre 2011. De plus en plus de parents sont touchés par le problème \" Je ne suis pas d' accord avec toutes les décisions de ce genre, je refuse de mettre de côté quelque chose du programme. Mais il ne fallait pas s' attendre à ce qu' il le dise : il avait raison. Nous sommes une démocratie représentative et cette démocratie est le socle sur lequel elle repose. Les deux hommes ont été mis en examen pour \" tentative d' assassinat \" et placés en détention provisoire, a précisé la même source. Dans un discours pour mettre fin aux hostilités dans la région, le président français, accompagné de la chancelière allemande, Angela Merkel, avait réaffirmé le droit du gouvernement malien à la sécurité des militaires français,\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vKi80k21GMAF","executionInfo":{"status":"ok","timestamp":1615885352902,"user_tz":-60,"elapsed":20592,"user":{"displayName":"François Guérillon","photoUrl":"","userId":"03664453875370395754"}},"outputId":"56fb29fe-0b71-4564-a2bb-e8d73ebde812"},"source":["# On prépare les données d'entraînement du générateur:\n","# Le générateur prédit le mot suivant pour chaque mot (ou plutôt token, en l'occurrence wordpiece) du texte fourni en entrée.\n","# Donc l'ensemble d'apprentissage est constitué de tuples (x=entrée, y=sortie) où y est un décalage de x d'un token vers la droite.\n","\n","# TO DO: train/test split\n","\n","batch_size = 16\n","train_size = int(0.75*len(L_reviews)/batch_size)*batch_size\n","print(\"Taille des batches :\", batch_size)\n","print(\"Nombre de lignes dans la base d'apprentissage :\", train_size)\n","\n","encodings = tokenizer(L_reviews, padding=True, return_tensors='tf')\n","input_ids = encodings.input_ids\n","attention_mask = encodings.attention_mask\n","x = {'input_ids': input_ids[:, :-1], 'attention_mask': attention_mask[:, :-1]}\n","y = input_ids[:, 1:]\n","assert x['input_ids'].shape == y.shape\n","assert x['attention_mask'].shape == y.shape\n","full_dataset = tf.data.Dataset.from_tensor_slices((x, y))\n","train_dataset = full_dataset.take(train_size).batch(batch_size).repeat()\n","test_dataset = full_dataset.skip(train_size).batch(batch_size)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Taille des batches : 16\n","Nombre de lignes dans la base d'apprentissage : 22992\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"q_jrufaFGMAJ"},"source":["# On \"encapsule\" le modèle initial dans un modèle qui ne renvoie que les \"logit\" des probabilités des mots,\n","# ce à quoi on peut appliquer la fonction de coût classique (entropie croisée)\n","class Pipotron(tf.keras.Model):\n","  def __init__(self, model):\n","    super().__init__(name=\"Pipotron\")\n","    self.gpt2 = model\n","  def __call__(self, input, training=False):    \n","    y = self.gpt2(input, training=training)\n","    return y.logits\n","\n","pipotron = Pipotron(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"35dvJh0rGMAK","executionInfo":{"status":"ok","timestamp":1615885353268,"user_tz":-60,"elapsed":17189,"user":{"displayName":"François Guérillon","photoUrl":"","userId":"03664453875370395754"}},"outputId":"721e4a74-2261-4468-945a-f6b22a0639ac"},"source":["# On contrôle les formats d'entrée et de sortie du modèle (à ce stade il est déboussolé par notre token de déclenchement):\n","for features, labels in train_dataset.take(1):\n","  i=4\n","  for k in features.keys():\n","    print(k, \":\", features[k].shape)\n","  print(tokenizer.decode(features['input_ids'][i], skip_special_tokens=False))\n","  print(\"labels :\", labels.shape)\n","  print(tokenizer.decode(labels[i], skip_special_tokens=False))\n","  z = pipotron(features)\n","  print(\"output :\", z.shape)\n","  print(tokenizer.decode(tf.math.argmax(z[i], axis=-1), skip_special_tokens=False))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["input_ids : (16, 250)\n","attention_mask : (16, 250)\n","<|review|> Un viognier fidèle à son bouquet exubérant, agréable et fluide. <|end|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>\n","labels : (16, 250)\n","Un viognier fidèle à son bouquet exubérant, agréable et fluide. <|end|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>\n","output : (16, 250, 50260)\n","' peudeurnier, à la terroir,érant,5, frais,Un.min nez nez nez--VbBBBBBBBBBBBBBBBvminminminminminminmintttttetetetetetevientvientvientvientbbbbvientbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbminminminminminminminminminminminminminminmintprBBBBBBBBBB'BBBB''B'BBBBBBBBBBBBBBBprprprprprprprbbbbminbminminminminminminminminminminminminminminminminminminminminminminminminminminminbbbbbbbminbbbbbbbbminbbtttttttttttttttttttttttttttttlég\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IMGyjNUEGMAM","executionInfo":{"status":"ok","timestamp":1615888483999,"user_tz":-60,"elapsed":3145288,"user":{"displayName":"François Guérillon","photoUrl":"","userId":"03664453875370395754"}},"outputId":"ecce5311-622d-431f-f8c7-2232531a7413"},"source":["# ENTRAINEMENT (fine tuning) DU MODELE :\n","\n","# TO DO : appliquer un learning_rate dégressif\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","pipotron.compile(optimizer=optimizer, loss=loss)\n","\n","# Pour voir la progression, on affiche le score à chaque 'round' (pseudo-epoch, qui est en fait une subdivision de la 'vraie' epoch\n","# si on considère qu'une vraie epoch consiste à passer toute la base d'entraînement une fois):\n","nb_epochs = 2\n","nb_steps_per_round = 400\n","nb_rounds = (nb_epochs*len(L_reviews))//(batch_size*nb_steps_per_round)\n","print(\"Nombre d'étapes (rounds) d'entraînement prévues :\", nb_rounds)\n","H_history = pipotron.fit(train_dataset, validation_data=test_dataset, epochs=nb_rounds, steps_per_epoch=nb_steps_per_round)\n","#pipotron.fit(train_dataset, epochs=nb_rounds, steps_per_epoch=nb_steps_per_round)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Nombre d'étapes (rounds) d'entraînement prévues : 9\n","Epoch 1/9\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","400/400 [==============================] - ETA: 0s - loss: 1.1703WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","400/400 [==============================] - 360s 871ms/step - loss: 1.1691 - val_loss: 0.5262\n","Epoch 2/9\n","400/400 [==============================] - 346s 866ms/step - loss: 0.5132 - val_loss: 0.4961\n","Epoch 3/9\n","400/400 [==============================] - 346s 867ms/step - loss: 0.4934 - val_loss: 0.4772\n","Epoch 4/9\n","400/400 [==============================] - 346s 867ms/step - loss: 0.4726 - val_loss: 0.4634\n","Epoch 5/9\n","400/400 [==============================] - 346s 866ms/step - loss: 0.4715 - val_loss: 0.4539\n","Epoch 6/9\n","400/400 [==============================] - 346s 866ms/step - loss: 0.4483 - val_loss: 0.4491\n","Epoch 7/9\n","400/400 [==============================] - 346s 866ms/step - loss: 0.4470 - val_loss: 0.4461\n","Epoch 8/9\n","400/400 [==============================] - 346s 866ms/step - loss: 0.4422 - val_loss: 0.4452\n","Epoch 9/9\n","400/400 [==============================] - 346s 866ms/step - loss: 0.4278 - val_loss: 0.4427\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P5fDlFatX5Im","executionInfo":{"status":"ok","timestamp":1615888485222,"user_tz":-60,"elapsed":3141235,"user":{"displayName":"François Guérillon","photoUrl":"","userId":"03664453875370395754"}},"outputId":"960368f6-d2e1-4ad7-af9c-ff2958e416f9"},"source":["# Sauvegarde du modèle:\r\n","import time\r\n","\r\n","fullpath = \"models/pipotron_\" + str(int(time.time()/60))\r\n","model.save_pretrained(fullpath)\r\n","print(\"MODEL SAVED AT :\", fullpath)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["MODEL SAVED AT : /drive/MyDrive/saved_models/pipotron_26931474\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uBDvdTW8GMAM","executionInfo":{"status":"ok","timestamp":1615893313982,"user_tz":-60,"elapsed":1892,"user":{"displayName":"François Guérillon","photoUrl":"","userId":"03664453875370395754"}},"outputId":"d6852f03-6978-4004-846f-2c6ad198c249"},"source":["# Rechargement du modèle, si besoin :\r\n","\r\n","fullpath = \"models/pipotron_26931474\"\r\n","model = TFGPT2LMHeadModel.from_pretrained(fullpath)\r\n","pipotron = Pipotron(model)"],"execution_count":101,"outputs":[{"output_type":"stream","text":["All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n","\n","All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at models/pipotron_26931474.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zficreSVxbBH","executionInfo":{"status":"ok","timestamp":1615893380056,"user_tz":-60,"elapsed":51397,"user":{"displayName":"François Guérillon","photoUrl":"","userId":"03664453875370395754"}},"outputId":"c5b70a61-a4d1-4953-f33d-4d478b846a75"},"source":["# On affiche quelques exemples de commentaires générés par le modèle entraîné:\r\n","print(pipote())\r\n","print(pipote())\r\n","print(pipote())\r\n"],"execution_count":102,"outputs":[{"output_type":"stream","text":["Un rouge vineux et solaire, dans un style mûr et fin, qui doit se fondre. Jean,, se débarrasse du carcan, avec ses arômes de prune et ses accents oxydatifs.\n","Un vin demi-sec que le terroir a préservé, qui s'exprime dans une matière finement parfumée. Agréable! Baugain..\n","Le parfum et la texture très soyeuse des tanins du Corton Le Charmes-Chambertin ne trompent pas : les arômes sont poudrés et la bouche, bien mûre, intègre le millésime.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6owyBwF9Mp6E"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MfURp6ppuNzX"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KZQLQvKduPIY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eskJ6G3QuOm4"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ZB1QxGzuO5e","executionInfo":{"status":"ok","timestamp":1615891656785,"user_tz":-60,"elapsed":1358,"user":{"displayName":"François Guérillon","photoUrl":"","userId":"03664453875370395754"}}},"source":[""],"execution_count":75,"outputs":[]}]}